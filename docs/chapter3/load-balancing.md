# 负载均衡概述

负载均衡是基于横向扩展保证高并发系统架构高可用的优化方法，是高可用网络基础架构的关键技术。

从网络分层角度来看可以分为两种：基于TCP/IP四层流量均衡、以及基于应用层协议的七层方案。 在域名解析上，也可以用DNS做负载均衡，不过由于比较明显的缺点，实际上已经很少使用。

四层负载均衡的方案中，原理是基于Linux内核对IP数据包路由分发的方式，比如经典的LVS就是在Linux内核中利用netfilter对IP包就行修改转发，以实现流量分发的负载均衡目标。在高可用性保证上，一般通过等价路由或者冗余路由协议，虚拟出一个VIP，内部实现多台双活或主备的方式。四层的方案仅作为流量入口处理IP数据包转发，无法识别上层协议，灵活性很差，但性能指标较好，一般用pps(包转发/秒)衡量性能。在大部分的方案中，通常是用LVS配合Keepalived实现前端入口，在配合后端7层的应用协议处理网关。


相对四层负载均衡，七层负载均衡的技术有点是可以识别应用层协议，可以通过对HTTP头、数据类型、请求类型做做复杂的逻辑处理，实现更灵活的控制。

比如七层负载均衡可以通过检查流经的HTTP报文头，根据报头内的URL、Cookie等等来实现请求的均衡。在此类方案的应用上，通常使用 Nginx 代理方案，配合Lua，如OpenResty，实现功能丰富且性能较高的网关方案。这类技术的底层使用 epoll以及LuaJIT相关的技术实现高并发的支撑，可以实现C10k的并发处理。一般用qps(请求/秒)衡量性能。


## 负载均衡应对流量的挑战

在网关常规的解决方案中，一般采用上游DNS指派分区服务器集群，然后服务器集群采用LVS + Nginx 的负载均衡方案，这种方式能应对几十万的QPS，也不需要太高的成本。

但这种方式在高流量的服务上，就不行了，以爱奇艺的服务为例，外界的可查的数据 DAU 差不多是 1.018亿，以一个用户10个请求为准，每天至少要处理10亿的请求，内部更是有数百亿的RPC等调用，处理这些请求总不能无限的堆砌资源，况且企业内部肯定也会考察用户成本的指标。


上面的技术成本问题，也带给我们一个新的思考方向： 10年前我们一直在解决C10的问题，而随着摩尔定律推动硬件性能的提升，在如10Core起CPU、DDR4、PCIe4.0等等技术升级下，也让我们可以考虑单机C10M的问题

### C10M的问题

C10M简单的理解就是：支持单机支持千万并发。它解决问题的范畴是如何让服务器处理更多的请求，如何提升单机极致性能。从广义角度可以理解为如何用更低成本、更高的效率带来更高技术服务价值，这也是C10X问题的价值意义。

在10多年前，我们谈论的多是C10K问题，通过使用io多路复用epoll、Coroutine、libevent库等技术，可以轻松搞定C10K问题。在一台普通配置的服务器部署Nginx、或者Redis服务可以轻松突破1万+的QPS性能指标。

<div  align="center">
	<p>图：C10K场景下的技术话题</p>
	<img src="/assets/chapter3/c10k.png" width = "500"  align=center />
</div> 


而在今天，移动互联网的发展、云计算的兴起，这在互联网基础设施上产生了很大的变革：越来越多的网络设备基础架构逐步向基于通用处理器平台的架构方向融合、传统物理网络到虚拟网络、扁平化的网络结构到基于SDN分层的网络结构等等变化，这在使得网络变得更加可控制和成本更低的同时，很少出现瓶颈的基础网络部分也开始要求能够支持大规模用户或应用程序的性能需求，以及海量数据的处理要求。

在上面的场景下，内部东西向规模性的Gbp/s数据流量，在关键节点的处理上，面临的挑战主要就是用户态协议栈和多核并发问题。

基于系统内核的数据传输会面临如 中断处理、内存拷贝、上下文切换、局部性失效、CPU亲和性、内存管理等等影响，在高并发的场景下，这些机制就是非常大的瓶颈所在。

<div  align="center">
	<p>图：多核下的CPU切换问题</p>
	<img src="/assets/chapter3/cpu.png" width = "400"  align=center />
</div> 

以CPU多核的性能问题来讨论：

大多数的程序在高于4核的CPU上不能发挥更好的性能，有时甚至会降低性能。其中主要的影响因素是CPU的缓存流水线。

L1 Cache 4个Cycles（CPU时钟），L2 Cache 12个Cycles，L3 30 Cycles，内存 300 Cycles。如果命中L1、L2缓存，性能则很高，若缓存Miss则性能则会降低。在高并发的情况下，因为上下文的切换过于频繁，缓存Miss的情况将大大增加。在上下文切换时，如果同一个连接的处理线程不是绑定在同一个CPU上的，那么将进一步加剧缓存Miss的情况。

在数据包处理瓶颈的解决思路上，得提一下 《C10M Defending The Internet At Scale》这篇文档，作者给到了较新奇的思路：系统内核不是解决 C10M 问题的办法，恰恰相反系统内核正式导致C10M问题的关键所在。

综合以上问题，如果作为高性能数据包处理服务器，内核本身就是一个非常大的瓶颈所在。这是由于UNIX系统设计初衷为电话网络的控制系统而设计的，而不是一般的服务器操作系统，所以它仅仅是一个数据负责数据传送的系统，没有所谓的控制层面和数据层面的说法，不适合处理大规模的网络数据包。

那很明显解决方案就是想办法绕过内核（Kernel-Bypass技术）。

Kernel-Bypass或者说是内核旁路，主要的思想是：

- 控制层和数据层分离：将数据包处理、内存管理、处理器调度等任务转移到用户空间去完成，而内核仅仅负责部分控制指令的处理。这样就不存在上述所说的系统中断、上下文切换、系统调用、系统调度等等问题。
- 使用多核编程技术代替多线程技术： 并设置 CPU 的亲和性，将线程和 CPU 核进行一比一绑定，减少彼此之间调度切换
- 针对 NUMA 系统：尽量使 CPU 核使用所在 NUMA 节点的内存，避免跨内存访问
- 使用大页内存：代替普通的内存，减少 Cache-Miss


近几年也出现了很多成熟的技术，如eBPF、Intel的DPDK，很多企业如Facebook的Katran、美团的MGW、爱奇艺的DPVS等都使用Intel 提供的DPDK套件进行kernel bypass，直接全部在用户态进行流量的处理。

爱奇艺的DPVS（爱奇艺负载均衡系统 DPDK+LVS）测试指标中：相较普通的LVS转发方案，pps转发能力提升六倍有余，这种方案也使得C10M的解决成为现实。

内核旁路的技术除了在负载均衡方面的应用，在云原生服务治理也绽放了相当做的光彩，因为无侵入、高性能等特性，在Kubernetes观测、监控和排错、高性能容器网络等也发展出了相当多的成熟应用。

但也要注意一个不能忽视的问题：kernel bypass提升性能之外，也大幅增大的系统的复杂度。 在Kubernetes的架构中，使用kube-proxy通过iptables规则就能简单明了的知晓各个服务的转发规则，而如果是换成基于eBPF的Cilium，数据包收发将变得极其复杂，对技术人员的要求也更高。