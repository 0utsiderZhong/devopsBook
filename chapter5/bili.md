# B站 2021年7月 故障分析

2021年7月13日 大量用户跑到微博反馈 B站 崩了，从晚上11点到第二天凌晨2点，这三个小时期间B站各项服务持续地完全不可用。

时隔一年之后，B站公开了详细的故障报告，笔者在基于B站官方的报告，在这里讲解B站故障的原因，从别人的故障复盘中学习经验以及总结教训。

## 故障现象

当日的晚 22:52 左右，B站 APP 开始持续出现内容无法加载、网页显示异常、收藏夹空白等异常现象。

彼时 B站的 运维人员也接到大量的服务和域名接入层不可用报警，从故障的表现特征，请求的 timeout 以及报警内容来看，初步定位应该 机房、四层LB、七层SLB的基础设施出现问题。


## 故障的处理

发生故障之后，运维人员发现SLB服务 cpu load 100%，怀疑SLB因流量过载不可用， 先通过 reload Server，不起作用，之后进行冷重启，依然不起作用。

此时紧急启用多活机房，由于多活机房配置复杂，只能恢复部分服务，整体服务性仍然存在故障。

运维人员并继续分析 SLB CPU 过载的问题。SLB的服务为 OpenResty ，通过 Perf 分析 热力图，定位在 `lua-resty-balancer` 模块中 `_gcd`函数。运维人员猜测与发版更新有关， 通过尝试代码回滚，依旧不可用。

此时只能紧急搭建全新的SLB服务，在两个小时的重建工作后，B站所有的服务恢复了正常，这也距离故障过去了三个小时。


## 故障背景及原因

B站的 SLB服务 为 OpenResty，并且基于 Lua lua-resty-balancer模块开发了服务发现功能。

在故障发生的前两个月，有业务提出想通过服务在注册中心的权重变更来实现SLB的动态调权，从而实现更精细的灰度能力。问题是：在某种发布模式中，应用的实例权重会短暂的调整为0，此时注册中心返回给SLB的权重是字符串类型的"0"。

此发布模式只有生产环境会用到，同时使用的频率极低，在SLB前期灰度过程中未触发此问题。

SLB 在balance_by_lua阶段，会将共享内存中保存的服务IP、Port、Weight 作为参数传给lua-resty-balancer模块用于选择upstream server，在节点 weight = "0" 时，balancer 模块中的 `_gcd` 函数收到的入参 b 可能为 "0"。

<div  align="center">
	<img src="/assets/chapter4/lua.png" width = "450"  align=center />
</div>

### 根因

- Lua 是动态类型语言，常用习惯里变量不需要定义类型，只需要为变量赋值即可。
- Lua在对一个数字字符串进行算术操作时，会尝试将这个数字字符串转成一个数字。
- 在 Lua 语言中，如果执行数学运算 n % 0，则结果会变为 nan（Not A Number）。
- `_gcd`函数对入参没有做类型校验，允许参数b传入："0"。同时因为"0" != 0，所以此函数第一次执行后返回是 `_gcd("0",nan)`。如果传入的是int 0，则会触发[ if b == 0 ]分支逻辑判断，不会死循环。
- `_gcd("0",nan)`函数再次执行时返回值是 `_gcd(nan,nan)`，然后 Nginx Worker 开始陷入死循环，进程 CPU 100%。


## 问题分析

### 为何故障刚发生时无法登陆内网后台？

事后复盘发现，用户在登录内网鉴权系统时，鉴权系统会跳转到多个域名下种登录的Cookie，其中一个域名是由故障的SLB代理的，受SLB故障影响当时此域名无法处理请求，导致用户登录失败。流程如下：

<div  align="center">
	<img src="/assets/chapter4/bili.png" width = "550"  align=center />
</div>

这个故障给我们的一个教训是：不同的业务、以及用户群体尽量使用不同的SLB服务，避免出现问题，影响全局不可用。在后续B站也 梳理了办公网系统的访问链路，跟用户链路隔离开，办公网链路不再依赖用户访问链路。

### 为何多活SLB在故障开始阶段也不可用？

多活SLB在故障时因CDN流量回源重试和用户重试，流量突增4倍以上，连接数突增100倍到1000W级别，导致这组SLB过载。后因流量下降和重启，逐渐恢复。

此SLB集群日常晚高峰CPU使用率30%左右，剩余Buffer不足两倍。如果多活SLB容量充足，理论上可承载住突发流量， 多活业务可立即恢复正常。

此处也可以看到，在发生机房级别故障时，多活是业务容灾止损最快的方案，这也是故障后重点投入治理的一个方向。

<div  align="center">
	<img src="/assets/chapter4/bili2.jpeg" width = "550"  align=center />
</div>

此时还应该关注一个问题：在重建服务时，用户会有大量的重试，如果在初期不进行有效的限流，依旧会大概率造成服务过载，需要一定的时间恢复


### 为何新建源站切流耗时这么久？

B站的公网架构如下：

<div  align="center">
	<img src="/assets/chapter4/bili3.png" width = "450"  align=center />
</div>

此处涉及三个团队：

- SLB团队：选择SLB机器、SLB机器初始化、SLB配置初始化
- 四层LB团队：SLB四层LB公网IP配置
- CDN团队：CDN更新回源公网IP、CDN切量

SLB的预案中只演练过SLB机器初始化、配置初始化，但和四层LB公网IP配置、CDN之间的协作并没有做过全链路演练，元信息在平台之间也没有联动，比如四层LB的Real Server信息提供、公网运营商线路、CDN回源IP的更新等。

所以一次完整的新建源站耗时非常久。

在事故后这一块的联动和自动化也是重点优化方向，在改进个新集群创建、初始化、四层LB公网IP配置已经能优化到5分钟以内。


## 优化改进

在故障之后，B站官方的技术人员也从多活、服务治理方面的总结和经验，我们也一起学习 他们在后续的改进。

### 多活建设

在故障的初期，做了多活的业务核心功能基本恢复正常，如APP推荐、APP播放、评论&弹幕拉取、动态、追番、影视等。

故障时直播业务也做了多活，但当晚没及时恢复的原因是：直播移动端首页接口虽然实现了多活，但没配置多机房调度。导致在主机房SLB不可用时直播APP首页一直打不开，非常可惜。通过这次事故，我们发现了多活架构存在的一些严重问题：

#### 多活基架能力不足

- 机房与业务多活定位关系混乱。
- CDN多机房流量调度不支持用户属性固定路由和分片。
- 业务多活架构不支持写，写功能当时未恢复。
- 部分存储组件多活同步和切换能力不足，无法实现多活。

#### 业务多活元信息缺乏平台管理

- 哪个业务做了多活？
- 业务是什么类型的多活，同城双活还是异地单元化？
- 业务哪些URL规则支持多活，目前多活流量调度策略是什么？
- 上述信息当时只能用文档临时维护，没有平台统一管理和编排。

#### 多活切量容灾能力薄弱

- 多活切量依赖CDN同学执行，其他人员无权限，效率低
- 无切量管理平台，整个切量过程不可视。
- 接入层、存储层切量分离，切量不可编排。
- 无业务多活元信息，切量准确率和容灾效果差。
- 我们之前的多活切量经常是这么一个场景：业务A故障了，要切量到多活机房。SRE跟研发沟通后确认要切域名A+URL A，告知CDN运维。CDN运维切量后研发发现还有个URL没切，再重复一遍上面的流程，所以导致效率极低，容灾效果也很差。 

所以我们多活建设的主要方向：

#### 多活基架能力建设

优化多活基础组件的支持能力，如数据层同步组件优化、接入层支持基于用户分片，让业务的多活接入成本更低。
重新梳理各机房在多活架构下的定位，梳理Czone、Gzone、Rzone业务域。
推动不支持多活的核心业务和已实现多活但架构不规范的业务改造优化。

#### 多活管控能力提升

统一管控所有多活业务的元信息、路由规则，联动其他平台，成为多活的元数据中心。
支持多活接入层规则编排、数据层编排、预案编排、流量编排等，接入流程实现自动化和可视化。
抽象多活切量能力，对接CDN、存储等组件，实现一键全链路切量，提升效率和准确率。
支持多活切量时的前置能力预检，切量中风险巡检和核心指标的可观测。
​​​
### SLB治理

#### 架构治理

故障前一个机房内一套SLB统一对外提供代理服务，导致故障域无法隔离。

后续SLB需按业务部门拆分集群，核心业务部门独立SLB集群和公网IP。跟CDN团队、四层LB&网络团队一起讨论确定SLB集群和公网IP隔离的管理方案。
明确SLB能力边界，非SLB必备能力，统一下沉到API Gateway，SLB组件和平台均不再支持，如动态权重的灰度能力。

#### 运维能力

SLB管理平台实现Lua代码版本化管理，平台支持版本升级和快速回滚。
SLB节点的环境和配置初始化托管到平台，联动四层LB的API，在SLB平台上实现四层LB申请、公网IP申请、节点上线等操作，做到全流程初始化5分钟以内。
SLB作为核心服务中的核心，在目前没有弹性扩容的能力下，30%的使用率较高，需要扩容把CPU降低到15%左右。
优化CDN回源超时时间，降低SLB在极端故障场景下连接数。同时对连接数做极限性能压测。

#### 自研能力

运维团队做项目有个弊端，开发完成自测没问题后就开始灰度上线，没有专业的测试团队介入。

此组件太过核心，需要引入基础组件测试团队，对SLB输入参数做完整的异常测试。

跟社区一起，Review使用到的OpenResty核心开源库源代码，消除其他风险。

基于Lua已有特性和缺陷，提升我们Lua代码的鲁棒性，比如变量类型判断、强制转换等。
招专业做LB的人。我们选择基于Lua开发是因为Lua简单易上手，社区有类似成功案例。团队并没有资深做Nginx组件开发的同学，也没有做C/C++开发的同学。
​​​
### 故障演练

本次事故中，业务多活流量调度、新建源站速度、CDN切量速度&回源超时机制均不符合预期。所以后续要探索机房级别的故障演练方案：

- 模拟CDN回源单机房故障，跟业务研发和测试一起，通过双端上的业务真实表现来验收多活业务的容灾效果，提前优化业务多活不符合预期的隐患。
- 灰度特定用户流量到演练的CDN节点，在CDN节点模拟源站故障，观察CDN和源站的容灾效果。
- 模拟单机房故障，通过多活管控平台，演练业务的多活切量止损预案。

### 应急响应

B站一直没有NOC/技术支持团队，在出现紧急事故时，故障响应、故障通报、故障协同都是由负责故障处理的SRE同学来承担。如果是普通事故还好，如果是重大事故，信息同步根本来不及。所以事故的应急响应机制必须优化：

- 优化故障响应制度，明确故障中故障指挥官、故障处理人的职责，分担故障处理人的压力。
- 事故发生时，故障处理人第一时间找backup作为故障指挥官，负责故障通报和故障协同。在团队里强制执行，让大家养成习惯。
- 建设易用的故障通告平台，负责故障摘要信息录入和故障中进展同步。

本次故障的诱因是某个服务使用了一种特殊的发布模式触发。我们的事件分析平台目前只提供了面向应用的事件查询能力，缺少面向用户、面向平台、面向组件的事件分析能力：

- 跟监控团队协作，建设平台控制面事件上报能力，推动更多核心平台接入。
- SLB建设面向底层引擎的数据面事件变更上报和查询能力，比如服务注册信息变更时某个应用的IP更新、weight变化事件可在平台查询。
- 扩展事件查询分析能力，除面向应用外，建设面向不同用户、不同团队、不同平台的事件查询分析能力，协助快速定位故障诱因。

