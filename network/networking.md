# 3.1 Linux 系统收包流程

高并发的系统架构中，微小的调整也可能引发连锁反应，这就需要对整个网络栈有系统的理解，本节将概述Linux系统接收数据包的过程，以帮助理解高并发系统面临的性能瓶颈以及相关的优化策略。

<div  align="center">
	<img src="../assets/networking.svg" width="650"  align=center />
	<p>图 3-4 Linux ingress 架构概览 </p>
</div>

如图3-4所示，Linux 系统收包流程如下。

1. 网卡 eth0 收到数据包。
2. 网卡通过 DMA 将数据包拷贝到内存的环形缓冲区(Ring Buffer，在网卡中有 RX Ring 和 TX Ring 两种缓冲)。
3. 数据从网卡拷贝到内存后, 网卡产生 IRQ（Interupt ReQuest，硬件中断）告知内核有新的数据包达到。
4. 内核收到中断后, 调用相应中断处理函数，开始唤醒 ksoftirqd 内核线程处理软中断。
5. 内核进行软中断处理，调用 NAPI poll 接口来获取内存环形缓冲区(ring buffer)的数据包，送至更上层处理。
6. 内核中网络协议栈：L2 处理。
7. 内核中网络协议栈：L3 处理。
8. 内核中网络协议栈：L4 处理。
9. 网络协议栈处理数据后，并将其发送到对应应用的 socket 接收缓冲区。

深入理解Linux系统接收数据包的流程后，我们便能更清晰地观察到在设计一个可以承载高并发流量的系统时，Linux内核带来的各种影响。例如，用户进程调用系统进入内核态的开销，CPU在响应数据包时产生的硬中断CPU开销，以及ksoftirqd内核线程因处理软中断而产生的上下文开销。

在移动互联网发达和云计算广泛应用的当下，传统的物理网络正在逐步向虚拟网络转变，扁平的网络结构也在朝向基于SDN的分层网络结构迈进。这些变化在降低我们的成本和加强控制力的同时，也对系统提出了更高的要求。那些过去不会成为瓶颈的网络基础部分，如今也面临着需要支持大规模、高并发数据处理的挑战。

下一节，我们继续了解Linux内核网络框架，以及跨内核技术等。