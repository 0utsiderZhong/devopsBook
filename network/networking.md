# 3.2 Linux 系统收包流程

高并发的系统架构中，任何细微调整，稍有不注意便会引起连锁反应，只有系统地了解整个网络栈，在处理疑难杂症或者系统优化工作中，才能做到手中有粮心中不慌。

在本节，我们概览一个 Linux 系统收包的流程，以便了解高并发系统所面临的性能瓶颈问题以及相关的优化策略。

<div  align="center">
	<img src="../assets/networking.svg" width="650"  align=center />
	<p>图 2-5 Linux ingress 架构概览 </p>
</div>

根据 图 2-5 示例，Linux 系统收包流程如下。

1. 网卡 eth0 收到数据包。
2. 网卡通过 DMA 将数据包拷贝到内存的环形缓冲区(Ring Buffer，在网卡中有 RX Ring 和 TX Ring 两种缓冲)。
3. 数据从网卡拷贝到内存后, 网卡产生 IRQ（Interupt ReQuest，硬件中断）告知内核有新的数据包达到。
4. 内核收到中断后, 调用相应中断处理函数，开始唤醒 ksoftirqd 内核线程处理软中断。
5. 内核进行软中断处理，调用 NAPI poll 接口来获取内存环形缓冲区(ring buffer)的数据包，送至更上层处理。
6. 内核中网络协议栈：L2 处理。
7. 内核中网络协议栈：L3 处理。
8. 内核中网络协议栈：L4 处理。
9. 网络协议栈处理数据后，并将其发送到对应应用的 socket 接收缓冲区。

## 1. 内核复杂的流程是高并发下瓶颈所在

理解 Linux 系统收包流程之后，我们就能明确知道收包相关的开销。
- 第一是用户进程调用系统调用陷入内核态的开销。
- 第二是 CPU 响应包的硬中断 CPU 开销
- 第三是 ksoftirqd 内核线程的软中断上下文开销。

总结基于系统内核的数据传输会面临如中断处理、内存拷贝、上下文切换、局部性失效、CPU 亲和性、内存管理等等影响，在高并发的场景下，这些机制就是非常大的瓶颈所在。

对于以上问题的解决方向有两类：一是优化相关的内核配置，使之尽可能适配当前场景，二是使用 XDP（eXpress Data Path，快速数据路径）、DPDK（data plane development kit，数据平面开发工具包）等跨内核技术，直接在用户态进行数据包处理。